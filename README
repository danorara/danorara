# coding=utf-8
import html
import re
import urllib.parse
from collections import OrderedDict
from datetime import datetime
from typing import Optional, List, Dict, Tuple

import requests
import xmltodict
from numpy import random

from pychroner import PluginMeta, PluginType

# 形態素解析後にやんないよツイートとして採用する連続する品詞
pass_rules: List[List[str]] = [
    ["接頭辞", "名詞"],
    ["名詞", "接尾辞"],
    ["形容詞", "名詞"],  # ~なxxx
    ["名詞", "特殊", "名詞"],
    ["助動詞,助動詞する", "名詞"],  # ~するxxx
    ["動詞", "名詞"],  # ~するxxx
    ["動詞", "助動詞,助動詞ない", "名詞"],  # ~しないxxx
    ["助詞,助詞連体化", "名詞"],  # のxxx
    ["形容詞,形容,連用テ接続", "動詞", "助動詞,助動詞た", "名詞"],  # ~な-したxxx
    ["名詞", "助詞,格助詞", "助詞,助詞連体化", "名詞"],  # xxxとのyyy
    ["動詞", "助詞,接続助詞,*,て,て,て"],  # ~して
    ["連体詞,連体", "名詞"],  # あのxxx
    ["形容詞,形容", "接尾辞,接尾さ"],  # ~さ,
    ["動詞", "助動詞,助動詞た", "名詞"],  # ~したxxx
    ["名詞", "助詞,格助詞,*,と,と,と", "動詞", "名詞"],  # xxxと~するyyy
    ["名詞", "助詞,格助詞,*,と,と,と", "名詞"],  # xxxとyyy
    ["名詞", "助詞,格助詞,*,に,に,に"],  # xxxに
    ["形容動詞,形動", "助動詞,助動詞だ,体言接続,な,な,だ", "名詞"],  # ~なxxx
    ["名詞", "助詞,並立助詞"],  # xxxとか
    ["助詞,格助詞", "名詞"],  # にxxx
    ["名詞", "助詞,格助詞", "動詞", "助詞,接続助詞"]
]

def normalizeText(tweet, bannedVia: List[str]) -> Optional[str]:
    """
    ツイートを正規化して形態素解析の精度を向上するための関数
    :param tweet: 正規化対象のツイート
    :param bannedVia: 禁止Via
    """
    # すでにふぁぼ済みや非公開アカウントのツイートを拾わない
    if tweet.favorited or tweet.user.protected:
        return
    # BANリストに入れてあるviaからのツイートは拾わない
    if tweet.source in bannedVia:
        return
    # ツイートの言語もユーザーの言語も日本語ではないなら拾わない
    if tweet.lang != "ja" and tweet.author.lang != "ja":
        return
    # 時報かbotを含むviaなら拾わない
    if "時報" in tweet.source or "bot" in tweet.source.lower():
        return
    # メンション/RTスキップ
    if "@" in tweet.text:
        return

    # 改行の削除
    text = tweet.text.replace("\r\n", "").replace("\n", "")
    URL_PATTERN = re.compile("http(s)://.+?(\s|$)")
    if URL_PATTERN.search(text):
        # URLが画像でなければスキップ
        if "media" not in tweet.entities:
            return
        # URLの削除
        text = URL_PATTERN.sub(r"\2", text)
    # HTML特殊文字のエスケープ
    text = html.unescape(text)

    return text

def getResult(text: str, appid: str) -> Optional[List[Dict]]:
    """
    Yahoo! テキスト解析の結果を取得する関数
    :param text: 解析対象のツイート
    :param appid: Yahoo! デベロッパーのApp ID
    """
    t = requests.get(f"https://jlp.yahooapis.jp/MAService/V1/parse?appid={appid}&results=ma,uniq&sentence={urllib.parse.quote(text)}&response=surface,reading,feature")
    if t.status_code != 200:
        return

    result = xmltodict.parse(t.text).get("ResultSet")
    if not result or isinstance(result["ma_result"]["word_list"]["word"], OrderedDict):
        return
    return result["ma_result"]["word_list"]["word"]

def checkStrict(i: int, data: List[Dict]) -> bool:
    """
    形態素解析の結果を厳密にチェックし品詞の前後関係を確認する関数
    :param i: 解析結果のインデックス
    :param data: 解析結果
    """
    for patterns in pass_rules:
        nodes = data[i - len(patterns) + 1:i] + data[i:i + len(patterns)]

        for j, node in enumerate(nodes):
            if node["feature"].startswith(patterns[0]):
                for x, y in zip(nodes[j + 1:], patterns[1:]):
                    if not x["feature"].startswith(y):
                        break
                else:
                    return True
            # 半分に折り返したらbreak
            if len([k for k in ["surface", "reading", "feature"] if node[k] == data[i][k]]) == 3:
                break
    return False

def filterWords(data: List[Dict], text: str, bannedWord: List[str]) -> Tuple[List[str], List[Dict]]:
    """
    形態素解析の結果を篩いにかけ採用するか判断する関数
    :param data: 解析結果
    :param text: 正規化済みツイート
    :param bannedWord: 禁止ワード
    """
    words = []
    nodes = []

    brackets, start_blacket = False, False
    for i, node in enumerate(data):
        # 特殊文字(改行等)ならcontinue
        if node["feature"].startswith("特殊,単漢"):
            continue
        # 空白を置換
        if node["feature"].startswith("特殊,空白"):
            node["surface"] = " "
        # 括弧の始まりを検知
        if node["feature"].startswith("特殊,括弧開"):
            brackets, start_blacket = True, True

        if not node["feature"].startswith("特殊,記号"):
            # 名詞でないなら前後の関係も厳密に吟味するワードフィルターにかける
            if not node["feature"].startswith("名詞") and not brackets and not checkStrict(i, data):
                continue

        # 禁止ワードフィルターに通す
        if node["surface"] in bannedWord:
            continue

        # ワードフィルターを通過した単語を追加
        # 前の名詞と連続している場合か括弧の連続は足し合わせて追加 (Need fix: 分割された単語が小さい場合は問題が発生することがある)
        if (words and words[-1] + node["surface"] in text) or brackets and not start_blacket:
            words[-1] += node["surface"]
        else:
            words.append(node["surface"])
            start_blacket = False
        # 括弧の終了を検知
        if node["feature"].startswith("特殊,括弧閉"):
            brackets = False
        # ノードの保管
        nodes.append(node)

    # 1文字なら破棄し, ハッシュタグが動作するように空白を両端につける
    return [" {} ".format(x) if x.startswith("#") else x for x in words if len(x) > 1], nodes

def generateImasNounDict() -> List[Dict[str, Optional[str]]]:
    with requests.get("https://imas-db.jp/misc/dic.txt") as r:
        r.encoding = "SHIFT_JISX0213"
        lines = [x.rstrip().split("\t") for x in r.text.split("\r\n") if x]

        result = []
        category = None
        firstCommentLine = True
        for i, x in enumerate(lines):
            if x[0].startswith("!"):
                if not firstCommentLine:
                    category = x[0].lstrip("! ")
                continue
            elif i > 0 and lines[i - 1][0].startswith("!"):
                firstCommentLine = False
            result.append({
                "surface": x[1],
                "reading": x[0],
                "feature": f"名詞,{x[2]}",
                "description": x[3],
                "category": category
            })
        return sorted(result, key=lambda x: x["reading"])

def combineWithDict(text: str, words: List[str], nodes: List, imasDict: List[Dict]) -> None:
    """
    強制結合ルールを使ってアイマス関連のワードを強制採用する関数
    :param text: 正規化済みツイート
    :param words: 採用候補のワード
    :param nodes: 品詞情報
    :param imasDict: アイマス辞書
    """
    # 3個追加して重み付け
    for noun in [x for x in imasDict if x["surface"] in text]:
        nodes.append({"surface": noun["surface"], "reading": noun["reading"], "feature": f"{noun['feature']},アイマス関連名詞"})
        [words.append(noun["surface"]) for _ in range(3)]

def choose(words: List[str], logger) -> Tuple:
    """
    乱数を使って採用候補のワードを抽選し投稿するツイートを生成する関数
    :param words: 採用候補のワード
    :param logger: PyChronerのロガー
    """
    r = random.rand()
    w = random.choice(words).strip()
    if 0.0 <= r < 0.03:
        text = f"{w}なら･･･♡"
    elif 0.03 <= r < 0.65:
        text = f"{w}しちゃダメです"
    else:
        text = f"{w}なんてダメです！"
    logger.info(f"chose \"{text}\" (r={r})")
    return text, r

@PluginMeta(PluginType.Schedule, multipleMinute=15, twitterAccount="MiriaYannaiyo")
def do(pluginApi):
    start = datetime.now()

   
    # 15回のみ再試行
    lastId: int = None
    for i in range(5):
        t = api.home_timeline(count=100, max_id=lastId)
        lastId = t[-1].id
        random.shuffle(t)

        for tweet in t:
            text = normalizeText(tweet, bannedVia)
            if not text:
                continue
            data = getResult(text, apiKey)
            if not data:
                continue
            words, nodes = filterWords(data, text, bannedWord)
            combineWithDict(tweet.text, words, nodes, imasDict)
            if not words:
                continue

            c = choose(words, logger)
            result = api.update_status(status=c[0])
            api.create_favorite(tweet.id)

            # 今回のツイートをDBに投げる
            finish = datetime.now()
            return db.3percent_miho.insert({
                "sec": (finish - start).total_seconds(),
                "datetime": finish.strftime("%m/%d %H:%M:%S"),
                "tweetLink": f"https://twitter.com/{tweet.author.screen_name}/status/{tweet.id}",
                "r": c[1],
                "chose": c[0],
                "words": words,
                "via": tweet.source,
                "original": tweet.text,
                "url": f"https://twitter.com/{result.author.screen_name}/status/{result.id}",
                "node": nodes
            })
